---
title: "IODS-final"
author: "Karin Helander, karin.a.helander@helsinki.fi"
date: "March 8, 2017"
output:
  html_document:
    theme: lume
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 6
    fig_height: 4
    code_folding: hide
---

```{r, include=FALSE}
library(ggplot2)
```



# Introduction

The purpose of this final assignment is to analyze tea consumption with R's own dataset "Tea" from the FactoMineR package. The research question follows; which factors are related to choosing a specific tea flavor? Soon we will notice, that because the data has many binary variables, we need to use the logistic regression in our tea-flavored analysis.

The logistic regression is a mostly used analytic tool when the resulting variable (in this case the tea flavor) is distributed as binary instead of a continuos distribution. Thuse, the linear regression model will not (and actually can not) produce any reliable results and so it needs a bit of updating. The logistic regression model is based on a logit distribution. The linearity is due to the logarithmic link function which converts the logit based function to a linear form. After we have estimated the model, the world of analysis by statistics and graphs is, to say, open.


# Structure of the data

The original 'Tea' dataset has 300 observations of 36 variables. It has questions about how the individuals drink their tea, what are their product's perception and also some personal details. For the analysis we want to have a slightly smaller set of variables, so we take a subset called 'tea_log'. The data wranling for this can be seen [here](https://github.com/karinhel/IODS-final/blob/master/wranling_final.R).

A good overview of the data is given by the structure of the dataset as well as the following graph. The data has 300 observations of 10 variables. In the data wranling we have also converted the variable 'Tea' to a so called dummy variable, where there are only two levels; "black" and "green". This 'teaclass' variable will be used for analysing differences between those how choose rather to drink black tea (where earl grey is now included) than green tea.

```{r}
tea_log <- read.table(file= "tea", header= T, sep=",")

str(tea_log)

```

```{r}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

```{r}
library(RColorBrewer);library(ggplot2)

g1 <- ggplot(tea_log, aes(x = age_Q, fill=Tea)) + geom_bar() + xlab("Age groups") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))+ scale_fill_brewer(
palette = "Set1") 

g2 <- ggplot(tea_log, aes(x = frequency, fill=Tea)) + geom_bar() + xlab("Frequency of tea drinking") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))+ scale_fill_brewer(
palette = "Set1") 

g3 <- ggplot(tea_log, aes(x = price, fill= Tea)) + geom_bar() + xlab("Price of tea") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))+ scale_fill_brewer(
palette = "Set1") 

g4 <- ggplot(tea_log, aes(x = how, fill= Tea)) + geom_bar() + xlab("The form of tea") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) + scale_fill_brewer(
palette = "Set1") 

multiplot(g1, g2, g3, g4, cols=2)
```

# Logistic regression analysis

As explaned earlier, the data has been reduced to 10 variables for the logistic regression analysis purpose. We are now going to form a model which aims to explain the choice of tea flavor (black or green) by different kind of explanatory factors.

```{r}
logmodel <- glm(teaclass ~ age_Q + how + price + frequency, data=tea_log, family = "binomial")

summary(logmodel)
```


In the p-value column we see that most of the variables that are chosen for the model are not that significant. Only one class in the grouped age variable as well as a class in the price variable have some kind of significance. Due to this, we will reduce the model to only one explanatory variables, which will be the classified age variable.

```{r}
logmodel2 <- glm(teaclass ~ age_Q, data=tea_log, family = "binomial")

summary(logmodel2)

```


Interpreting the estimated coefficients is a bit more complex than in linear regression, because now we have a binary outcome variable. The backroud for this is that, to make our model linear, we need to fix it with some calculation. The logistic regression model is based on a logistic function and thus the result variable values are actually values of the link function. That is basically the log of odds. This results in the fact that coefficient values can be interpret as odds ratios when we take the exponential of a single coefficient.

So, in the case of the fitted model we could say first of all, that the intercept tells again the level of the model. This works similarly for linear and logistic models. But then the real coefficients. Generally, when we have a multiple regression model, we interpret the coeff.s to be the change in the outcome variable when there is a one-unit change in the explanatory variable in question. Also important thing is, that when we do this, we assume the other variables to be constant, in other words we eliminate the effect of any other variable in the interpretation.
Now in our logistic regression with a dichotomous outcome, the fitted model tells how much more likely is it to have the value 1 of "teaclass" than 0, when a single explanatory variable is in question and others are constant. So,

* when comparing the variable "age_Q"'s group 15-24yrs to the controll group, the odds for "teaclass" to be 1, which means odds for drinking black tea decreases by 1.60.

* when comparing the variable "age_Q"'s group 25-34yrs to the controll group, the odds for drinking black tea decreases only by 0.29.

and so on. It seems that younger and 35-44 aged individuals drink less black tea than the oldest and 25-34 year-old respondents. In general R is now showing us the value of coeff. when we compare it to a controll group that R has selected automatically. The real value of this coeff. is calculated: intercept - the value.


After the model has been fit, we can calculate some odds ratios as well as confidence intervals for them.

```{r, message=FALSE}
library(tidyr)

OR <- coef(logmodel2) %>% exp

CI <- confint(logmodel2) %>% exp

cbind(OR, CI)

```


Odds ratios are exponentials of the coefficient values. For example the "age_Q" variable's class 15-24yrs has an OR value of 0.201. Compared to the control group it is only 0.201 more likely to have a "true" value of teaclass than for the variable value. This means that when comparing the 15-24 age group, the alcohol consumption of the observed is over to times more likely to be higher.

For any of the classes of the age variable in the model the OR values are not that high. Overall it seems that the choice of tea is not due to any factors included in the dataset and can be rather emotional than due to some real features, to say.

